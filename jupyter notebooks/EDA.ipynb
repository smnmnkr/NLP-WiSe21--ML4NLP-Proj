{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Loading the data\n",
    "train_raw = pd.read_csv('../data/train.csv', sep=',')\n",
    "test_raw = pd.read_csv('../data/eval.csv', sep=',')\n",
    "\n",
    "train_raw.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_raw[train_raw[\"lang\"]!=\"en\"].head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check null values and n_unique\n",
    "pd.concat([train_raw.isnull().sum(), train_raw.nunique()], axis=1).rename(columns = {0: 'is_null', 1: 'n_unique' })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = English()\n",
    "tokenizer = Tokenizer(nlp.vocab)\n",
    "\n",
    "# train_raw= train_raw.drop(columns=['id','time','smth'])\n",
    "train_raw['sent'] = train_raw['sent'].map(lambda label: True if label == 'Neutral' else False)\n",
    "train_raw['len_tweet'] = train_raw['tweet'].map(lambda tweet: len(tweet))\n",
    "train_raw['tokens'] = train_raw['tweet'].map(lambda tweet: [tok.text for tok in tokenizer(tweet)])\n",
    "train_raw['n_tokens'] = train_raw['tokens'].map(lambda tokens: len(tokens))\n",
    "train_raw['n_urls'] = train_raw['tokens'].map(lambda tokens: len( [tok for tok in tokens if 'http' in tok] ))\n",
    "\n",
    "\n",
    "train_raw.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(1,3,1)\n",
    "sns.histplot(data=train_raw,x='n_tokens',kde=True, bins=20,hue='sent')\n",
    "plt.subplot(1,3,2)\n",
    "sns.histplot(data=train_raw,x='len_tweet',kde=True, bins=20,hue='sent')\n",
    "plt.subplot(1,3,3)\n",
    "sns.histplot(data=train_raw,x='n_urls',kde=True, bins=20,hue='sent')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap for later use\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.set(font_scale=1.5)\n",
    "sns.heatmap(train_raw.corr(), cmap='coolwarm', annot=True, annot_kws={'size':15})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This is for spell checking, might be useful\n",
    "\n",
    "# from tqdm import tqdm\n",
    "# import pkg_resources\n",
    "# from symspellpy import SymSpell, Verbosity\n",
    "\n",
    "# sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\n",
    "# dictionary_path = pkg_resources.resource_filename(\n",
    "#     \"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
    "# bigram_path = pkg_resources.resource_filename(\n",
    "#     \"symspellpy\", \"frequency_bigramdictionary_en_243_342.txt\")\n",
    "# # term_index is the column of the term and count_index is the\n",
    "# # column of the term frequency\n",
    "# sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
    "# sym_spell.load_bigram_dictionary(bigram_path, term_index=0, count_index=2)\n",
    "\n",
    "# def check_spell(input_term):\n",
    "#     # lookup suggestions for multi-word input strings (supports compound\n",
    "#     # splitting & merging)\n",
    "#     # max edit distance per lookup (per single word, not per whole input string)\n",
    "#     suggestions = sym_spell.lookup_compound(input_term, max_edit_distance=2)\n",
    "#     # display suggestion term, edit distance, and term frequency\n",
    "#     pbar.update()\n",
    "#     return suggestions[0]._term\n",
    "\n",
    "# pbar = tqdm(total=len(train['search_term']))\n",
    "# train['search_term'] = train['search_term'].map(check_spell)\n",
    "# pbar.close()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a97517a1f2e44bae43a0d1adced36113c0aa0bace4a2569b7d21fd338b4eff8a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
