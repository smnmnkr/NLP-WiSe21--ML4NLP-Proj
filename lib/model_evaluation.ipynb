{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference: https://mccormickml.com/2019/07/22/BERT-fine-tuning/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual training and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import train_model_on_train_data\n",
    "\n",
    "\n",
    "# Parameters\n",
    "TRAIN_DATA_PATH = \"../data/train.csv\"\n",
    "MODEL_NAME = 'microsoft/deberta-base'\n",
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 1\n",
    "\n",
    "model, training_stats = train_model_on_train_data(TRAIN_DATA_PATH, MODEL_NAME, BATCH_SIZE, NUM_EPOCHS)\n",
    "\n",
    "training_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "df_training = pd.DataFrame(training_stats)\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.set(style='darkgrid')\n",
    "sns.set(font_scale=1.5)\n",
    "\n",
    "plt.plot(df_training['training_loss'], 'b-o', label=\"Training\")\n",
    "plt.plot(df_training['validation_loss'], 'g-o', label=\"Validation\")\n",
    "\n",
    "plt.title(\"Training & Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.xticks([1, 2, 3, 4])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import evaluate_on_test_data\n",
    "\n",
    "\n",
    "TEST_DATA_PATH = \"../data/eval.csv\"\n",
    "\n",
    "\n",
    "testing_stats = evaluate_on_test_data(model, TEST_DATA_PATH, MODEL_NAME, BATCH_SIZE)\n",
    "\n",
    "testing_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving Model F1 Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load results_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "results_dictionary = {\n",
    "        \"model_name\": [],\n",
    "        \"pipeline\": [],\n",
    "        \"training_loss\": [],\n",
    "        \"validation_loss\": [],\n",
    "        \"validation_f1\": [],\n",
    "        \"test_loss\": [],\n",
    "        \"test_f1\": [],\n",
    "        \"augmented\": []\n",
    "    }\n",
    "\n",
    "\n",
    "old_df = pd.read_csv(\"../results/04-DeBERTa-base many configurations/2nd_models_comparison.csv\")\n",
    "\n",
    "for i in range(len(old_df)):\n",
    "    aa = old_df.iloc[i].to_dict()\n",
    "\n",
    "    for k,v in aa.items():\n",
    "        results_dictionary[k].append(v)\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "results_df = pd.DataFrame(results_dictionary)\n",
    "results_df.head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dictionary = {\n",
    "        \"model_name\": [],\n",
    "        \"pipeline\": [],\n",
    "        \"training_loss\": [],\n",
    "        \"validation_loss\": [],\n",
    "        \"validation_f1\": [],\n",
    "        \"test_loss\": [],\n",
    "        \"test_f1\": []\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "\n",
    "from data_preparation import _load_dataset, _prepare_data, _create_dataloaders, _create_tensors\n",
    "from helper_functions import get_device\n",
    "from model_preparation import Model, set_seed\n",
    "\n",
    "\n",
    "# Parameters\n",
    "TRAIN_DATA_PATH = \"../data/train.csv\"\n",
    "TEST_DATA_PATH = \"../data/eval.csv\"\n",
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 1\n",
    "SEED = 42\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "device = get_device()\n",
    "\n",
    "\n",
    "# 'microsoft/deberta-v2-xlarge'\n",
    "# \"bert-base-cased\"\n",
    "# \"bert-base-uncased\"\n",
    "for MODEL_NAME in ['microsoft/deberta-base']:\n",
    "    for pipeline in [['hyperlinks', 'mentions', 'hashtags', 'retweet', 'repetitions', 'emojis', 'smileys', 'spaces']\n",
    "                    ]:\n",
    "        \n",
    "        if TRAIN_DATA_PATH == \"../data/train_aug.csv\":\n",
    "            df = _load_dataset(TRAIN_DATA_PATH)\n",
    "        else:\n",
    "            df = _load_dataset(TRAIN_DATA_PATH)\n",
    "            df = _prepare_data(df, pipeline)\n",
    "\n",
    "        input_ids, attention_masks, labels = _create_tensors(df, MODEL_NAME)\n",
    "        train_dataloader, validation_dataloader = _create_dataloaders(input_ids, attention_masks, labels, BATCH_SIZE, \n",
    "                                                                    create_validation_set= True)\n",
    "\n",
    "        df = _load_dataset(TEST_DATA_PATH)\n",
    "        df = _prepare_data(df, pipeline)\n",
    "        input_ids, attention_masks, labels = _create_tensors(df, MODEL_NAME)\n",
    "        test_dataloader = _create_dataloaders(input_ids, attention_masks, labels, BATCH_SIZE, create_validation_set= False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "        model_class = Model(MODEL_NAME, NUM_EPOCHS, len(train_dataloader))\n",
    "        model, optimizer, lr_scheduler = model_class.get_model_optimizer_scheduler()\n",
    "        model = model.to(device)\n",
    "\n",
    "        training_loss = 0\n",
    "        val_loss = 0\n",
    "        val_f1 = 0\n",
    "\n",
    "        training_stats = []\n",
    "        try:\n",
    "            for epoch in range(NUM_EPOCHS):\n",
    "                print(f\"EPOCH {epoch+1}/{NUM_EPOCHS}\\n\")\n",
    "                model.train()\n",
    "                total_train_loss = 0\n",
    "\n",
    "                for step, batch in enumerate(train_dataloader):\n",
    "                    model.zero_grad()\n",
    "                    parameters = {\n",
    "                        \"input_ids\" : batch[0].to(device),\n",
    "                        \"attention_mask\" :  batch[1].to(device), \n",
    "                        \"labels\" : batch[2].to(device)\n",
    "                    }\n",
    "                    outputs = model(**parameters)\n",
    "\n",
    "                    loss = outputs.loss\n",
    "                    total_train_loss += loss.item()\n",
    "                    loss.backward()\n",
    "\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "                    optimizer.step()\n",
    "                    lr_scheduler.step()\n",
    "                    optimizer.zero_grad()\n",
    "                    # progress_bar.update(1)\n",
    "\n",
    "                    if step % 100 == 0 and step != 0:\n",
    "                        print(f\"BATCH {step}/{len(train_dataloader)}:\\tTraining loss({loss.item()})\")\n",
    "\n",
    "                training_stats.append({\n",
    "                    \"epoch\":epoch+1,\n",
    "                    \"training_loss\":total_train_loss/len(train_dataloader)\n",
    "                    })\n",
    "\n",
    "                total_val_loss = 0\n",
    "                metric = load_metric(\"f1\")\n",
    "\n",
    "                model.eval()\n",
    "                for batch in validation_dataloader:\n",
    "\n",
    "                    parameters = {\n",
    "                        \"input_ids\" : batch[0].to(device),\n",
    "                        \"attention_mask\" :  batch[1].to(device), \n",
    "                        \"labels\" : batch[2].to(device)\n",
    "                    }\n",
    "                    with torch.no_grad():\n",
    "                        outputs = model(**parameters)\n",
    "\n",
    "                    logits = outputs.logits\n",
    "                    loss = outputs.loss\n",
    "                    total_val_loss += loss.item()\n",
    "\n",
    "                    predictions = torch.argmax(logits, dim=-1)\n",
    "                    metric.add_batch(predictions=predictions, references=parameters[\"labels\"])\n",
    "\n",
    "                training_stats[epoch][\"validation_loss\"] = total_val_loss/len(validation_dataloader)\n",
    "                training_stats[epoch][\"validation_f1_score\"] = metric.compute()\n",
    "\n",
    "                print(f\"\\nAvg training loss:    {training_stats[epoch]['training_loss']}\")\n",
    "                print(f\"Avg validation loss:  {training_stats[epoch]['validation_loss']}\")\n",
    "                print(f\"F1 validation score:  {training_stats[epoch]['validation_f1_score']}\\n\")\n",
    "\n",
    "                training_loss = training_stats[epoch]['training_loss']\n",
    "                val_loss = training_stats[epoch]['validation_loss']\n",
    "                val_f1 = training_stats[epoch]['validation_f1_score']\n",
    "\n",
    "        except RuntimeError as e:\n",
    "            print(e)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        model = model.to(device)\n",
    "        testing_stats = []\n",
    "\n",
    "        try:\n",
    "            total_test_loss = 0\n",
    "            metric = load_metric(\"f1\")\n",
    "\n",
    "            model.eval()\n",
    "\n",
    "            for n, batch in enumerate(test_dataloader):\n",
    "\n",
    "                parameters = {\n",
    "                    \"input_ids\" : batch[0].to(device),\n",
    "                    \"attention_mask\" :  batch[1].to(device), \n",
    "                    \"labels\" : batch[2].to(device)\n",
    "                }\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(**parameters)\n",
    "                \n",
    "                logits = outputs.logits\n",
    "                loss = outputs.loss\n",
    "                total_test_loss += loss.item()\n",
    "\n",
    "                predictions = torch.argmax(logits, dim=-1)\n",
    "                metric.add_batch(predictions=predictions, references=parameters[\"labels\"])\n",
    "\n",
    "            testing_stats.append({\n",
    "                \"test_loss\": total_test_loss/len(test_dataloader),\n",
    "                \"test_f1_score\": metric.compute()\n",
    "            })\n",
    "\n",
    "            print(f\"\\nAvg test loss:  {testing_stats[0]['test_loss']}\")\n",
    "            print(f\"F1 test score:  {testing_stats[0]['test_f1_score']}\\n\")\n",
    "\n",
    "\n",
    "            results_dictionary[\"model_name\"].append(MODEL_NAME)\n",
    "            results_dictionary[\"pipeline\"].append(str(pipeline))\n",
    "            results_dictionary[\"training_loss\"].append(training_loss)\n",
    "            results_dictionary[\"validation_loss\"].append(val_loss)\n",
    "            results_dictionary[\"validation_f1\"].append(val_f1)\n",
    "            results_dictionary[\"test_loss\"].append(testing_stats[0]['test_loss'])\n",
    "            results_dictionary[\"test_f1\"].append(testing_stats[0]['test_f1_score'])\n",
    "            if TRAIN_DATA_PATH == \"../data/train_aug.csv\":\n",
    "                results_dictionary[\"augmented\"].append(\"yes\")\n",
    "            else:\n",
    "                results_dictionary[\"augmented\"].append(\"no\")\n",
    "\n",
    "\n",
    "        except RuntimeError as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "results_df = pd.DataFrame(results_dictionary)\n",
    "results_df.to_csv(\"../results/04-DeBERTa-base many configurations/2nd_models_comparison.csv\", index=False)\n",
    "results_df.head(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check error cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import train_model_on_train_data\n",
    "\n",
    "\n",
    "# Parameters\n",
    "TRAIN_DATA_PATH = \"../data/train.csv\"\n",
    "MODEL_NAME = 'microsoft/deberta-base'\n",
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 1\n",
    "\n",
    "model, training_stats = train_model_on_train_data(TRAIN_DATA_PATH, MODEL_NAME, BATCH_SIZE, NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from model_preparation import set_seed\n",
    "from data_preparation import _load_dataset, _prepare_data, _create_dataloaders, _create_tensors\n",
    "\n",
    "\n",
    "data_path = \"../data/eval.csv\"\n",
    "model_name = 'microsoft/deberta-base'\n",
    "batch_size = 16\n",
    "create_validation_set = False\n",
    "SEED = 42\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "\n",
    "df = _load_dataset(data_path)\n",
    "\n",
    "final_df = df[\"tweet\"].copy()\n",
    "\n",
    "pipeline = ['hyperlinks', 'mentions', 'hashtags', 'retweet', 'repetitions', 'emojis', 'smileys', 'spaces']\n",
    "df = _prepare_data(df, pipeline)\n",
    "\n",
    "final_df = pd.concat([final_df, df[[\"text\",\"label\"]]], axis = 1)\n",
    "\n",
    "input_ids, attention_masks, labels = _create_tensors(df, model_name)\n",
    "dataloaders = _create_dataloaders(input_ids, attention_masks, labels, batch_size, create_validation_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "\n",
    "from helper_functions import get_device\n",
    "\n",
    "\n",
    "device = get_device()\n",
    "model = model.to(device)\n",
    "\n",
    "pred_list = np.array([])\n",
    "\n",
    "test_dataloader = dataloaders\n",
    "\n",
    "testing_stats = []\n",
    "\n",
    "try:\n",
    "    total_test_loss = 0\n",
    "    metric = load_metric(\"f1\")\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    for n, batch in enumerate(test_dataloader):\n",
    "\n",
    "        parameters = {\n",
    "            \"input_ids\" : batch[0].to(device),\n",
    "            \"attention_mask\" :  batch[1].to(device), \n",
    "            \"labels\" : batch[2].to(device)\n",
    "        }\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**parameters)\n",
    "        \n",
    "        logits = outputs.logits\n",
    "        loss = outputs.loss\n",
    "        total_test_loss += loss.item()\n",
    "\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        metric.add_batch(predictions=predictions, references=parameters[\"labels\"])\n",
    "\n",
    "        if input_ids[n*32:(n+1)*32].equal(parameters[\"input_ids\"].cpu()):\n",
    "            pred_list = np.append(pred_list,predictions.cpu().numpy())\n",
    "\n",
    "    testing_stats.append({\n",
    "        \"test_loss\": total_test_loss/len(test_dataloader),\n",
    "        \"test_f1_score\": metric.compute()\n",
    "    })\n",
    "\n",
    "    print(f\"\\nAvg test loss:  {testing_stats[0]['test_loss']}\")\n",
    "    print(f\"F1 test score:  {testing_stats[0]['test_f1_score']}\\n\")\n",
    "\n",
    "except RuntimeError as e:\n",
    "    print(e)\n",
    "\n",
    "final_df[\"prediction\"] = pred_list\n",
    "final_df[\"match\"] = final_df.apply(lambda row: \"\" if row[\"label\"] == row[\"prediction\"] else \"NO\", axis=1)\n",
    "\n",
    "# final_df.to_csv(\"predictions.csv\",index=False)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "final_df[final_df[\"match\"]==\"NO\"].head(40)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a97517a1f2e44bae43a0d1adced36113c0aa0bace4a2569b7d21fd338b4eff8a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
