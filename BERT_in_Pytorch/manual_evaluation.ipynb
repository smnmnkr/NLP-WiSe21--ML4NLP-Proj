{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference: https://mccormickml.com/2019/07/22/BERT-fine-tuning/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual training and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import train_model_on_train_data\n",
    "\n",
    "\n",
    "# Parameters\n",
    "TRAIN_DATA_PATH = \"../data/train.csv\"\n",
    "MODEL_NAME = \"bert-base-cased\"\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 1\n",
    "\n",
    "model, training_stats = train_model_on_train_data(TRAIN_DATA_PATH, MODEL_NAME, BATCH_SIZE, NUM_EPOCHS)\n",
    "\n",
    "training_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "df_training = pd.DataFrame(training_stats)\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.set(style='darkgrid')\n",
    "sns.set(font_scale=1.5)\n",
    "\n",
    "plt.plot(df_training['training_loss'], 'b-o', label=\"Training\")\n",
    "plt.plot(df_training['validation_loss'], 'g-o', label=\"Validation\")\n",
    "\n",
    "plt.title(\"Training & Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.xticks([1, 2, 3, 4])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import evaluate_on_test_data\n",
    "\n",
    "\n",
    "TEST_DATA_PATH = \"../data/eval.csv\"\n",
    "\n",
    "\n",
    "testing_stats = evaluate_on_test_data(model, TEST_DATA_PATH, MODEL_NAME, BATCH_SIZE)\n",
    "\n",
    "testing_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Lisandro\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Lisandro\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\omw-1.4.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://github.com/jasonwei20/eda_nlp\n",
    "import nltk\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "original_df = pd.read_csv(\"data_before_going_to_model.csv\")\n",
    "\n",
    "original_df.drop(['tweet'], axis=1, inplace=True)\n",
    "original_df.rename(columns={'text': 'sentence', 'sent': 'label'}, inplace=True)\n",
    "original_df = original_df[[\"label\", \"sentence\"]]\n",
    "original_df.to_csv(\"eda_nlp-master/data/original_train.txt\", index=False, header=False, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated augmented sentences with eda for original_train.txt to augmented_train.txt with num_aug=9\n"
     ]
    }
   ],
   "source": [
    "!python eda_nlp-master/code/augment.py --input=original_train.txt --output=augmented_train.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "augmented_df = pd.read_csv(\"augmented_train.txt\", names = [\"label\", \"text\"], sep = \"\\t\")\n",
    "augmented_df = augmented_df.sample(frac=1).reset_index(drop=True)\n",
    "augmented_df.to_csv(\"../data/train_aug.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving Model F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>pipeline</th>\n",
       "      <th>training_loss</th>\n",
       "      <th>validation_loss</th>\n",
       "      <th>validation_f1</th>\n",
       "      <th>test_loss</th>\n",
       "      <th>test_f1</th>\n",
       "      <th>augmented</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>microsoft/deberta-base</td>\n",
       "      <td>['hyperlinks', 'mentions', 'hashtags', 'retweet', 'repetitions', 'emojis', 'smileys', 'spaces']</td>\n",
       "      <td>0.539779</td>\n",
       "      <td>0.498220</td>\n",
       "      <td>{'f1': 0.7855407047387605}</td>\n",
       "      <td>0.510469</td>\n",
       "      <td>{'f1': 0.7723367697594502}</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>microsoft/deberta-base</td>\n",
       "      <td>['lowercase', 'hyperlinks', 'mentions', 'hashtags', 'retweet', 'repetitions', 'emojis', 'smileys', 'spaces']</td>\n",
       "      <td>0.560275</td>\n",
       "      <td>0.507515</td>\n",
       "      <td>{'f1': 0.7865235539654144}</td>\n",
       "      <td>0.519109</td>\n",
       "      <td>{'f1': 0.7712455516014236}</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>microsoft/deberta-base</td>\n",
       "      <td>['hyperlinks', 'mentions', 'hashtags', 'repetitions', 'emojis', 'smileys', 'spaces']</td>\n",
       "      <td>0.544365</td>\n",
       "      <td>0.501752</td>\n",
       "      <td>{'f1': 0.7833935018050541}</td>\n",
       "      <td>0.512311</td>\n",
       "      <td>{'f1': 0.7747542384955122}</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>microsoft/deberta-base</td>\n",
       "      <td>['hyperlinks', 'mentions', 'hashtags', 'retweet', 'emojis', 'smileys', 'spaces']</td>\n",
       "      <td>0.555020</td>\n",
       "      <td>0.518818</td>\n",
       "      <td>{'f1': 0.7694189602446483}</td>\n",
       "      <td>0.504441</td>\n",
       "      <td>{'f1': 0.777521613832853}</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               model_name  \\\n",
       "0  microsoft/deberta-base   \n",
       "1  microsoft/deberta-base   \n",
       "2  microsoft/deberta-base   \n",
       "3  microsoft/deberta-base   \n",
       "\n",
       "                                                                                                       pipeline  \\\n",
       "0               ['hyperlinks', 'mentions', 'hashtags', 'retweet', 'repetitions', 'emojis', 'smileys', 'spaces']   \n",
       "1  ['lowercase', 'hyperlinks', 'mentions', 'hashtags', 'retweet', 'repetitions', 'emojis', 'smileys', 'spaces']   \n",
       "2                          ['hyperlinks', 'mentions', 'hashtags', 'repetitions', 'emojis', 'smileys', 'spaces']   \n",
       "3                              ['hyperlinks', 'mentions', 'hashtags', 'retweet', 'emojis', 'smileys', 'spaces']   \n",
       "\n",
       "   training_loss  validation_loss               validation_f1  test_loss  \\\n",
       "0       0.539779         0.498220  {'f1': 0.7855407047387605}   0.510469   \n",
       "1       0.560275         0.507515  {'f1': 0.7865235539654144}   0.519109   \n",
       "2       0.544365         0.501752  {'f1': 0.7833935018050541}   0.512311   \n",
       "3       0.555020         0.518818  {'f1': 0.7694189602446483}   0.504441   \n",
       "\n",
       "                      test_f1 augmented  \n",
       "0  {'f1': 0.7723367697594502}        no  \n",
       "1  {'f1': 0.7712455516014236}        no  \n",
       "2  {'f1': 0.7747542384955122}        no  \n",
       "3   {'f1': 0.777521613832853}        no  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "results_dictionary = {\n",
    "        \"model_name\": [],\n",
    "        \"pipeline\": [],\n",
    "        \"training_loss\": [],\n",
    "        \"validation_loss\": [],\n",
    "        \"validation_f1\": [],\n",
    "        \"test_loss\": [],\n",
    "        \"test_f1\": [],\n",
    "        \"augmented\": []\n",
    "    }\n",
    "\n",
    "\n",
    "old_df = pd.read_csv(\"2nd_models_comparison.csv\")\n",
    "\n",
    "for i in range(len(old_df)):\n",
    "    aa = old_df.iloc[i].to_dict()\n",
    "\n",
    "    for k,v in aa.items():\n",
    "        results_dictionary[k].append(v)\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "results_df = pd.DataFrame(results_dictionary)\n",
    "results_df.head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_dictionary = {\n",
    "#         \"model_name\": [],\n",
    "#         \"pipeline\": [],\n",
    "#         \"training_loss\": [],\n",
    "#         \"validation_loss\": [],\n",
    "#         \"validation_f1\": [],\n",
    "#         \"test_loss\": [],\n",
    "#         \"test_f1\": []\n",
    "#     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: NVIDIA GeForce RTX 2070 with Max-Q Design\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.weight', 'pooler.dense.weight', 'pooler.dense.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1/1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lisandro\\Desktop\\sentiment-challenge\\venv\\lib\\site-packages\\transformers\\models\\deberta\\modeling_deberta.py:1207: UserWarning: This overload of nonzero is deprecated:\n",
      "\tnonzero()\n",
      "Consider using one of the following signatures instead:\n",
      "\tnonzero(*, bool as_tuple) (Triggered internally at  ..\\torch\\csrc\\utils\\python_arg_parser.cpp:882.)\n",
      "  label_index = (labels >= 0).nonzero()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BATCH 100/3562:\tTraining loss(0.7088796496391296)\n",
      "BATCH 200/3562:\tTraining loss(0.5513307452201843)\n",
      "BATCH 300/3562:\tTraining loss(0.4055532217025757)\n",
      "BATCH 400/3562:\tTraining loss(0.7488111257553101)\n",
      "BATCH 500/3562:\tTraining loss(0.3334040641784668)\n",
      "BATCH 600/3562:\tTraining loss(0.5062378644943237)\n",
      "BATCH 700/3562:\tTraining loss(0.6252745985984802)\n",
      "BATCH 800/3562:\tTraining loss(0.43599385023117065)\n",
      "BATCH 900/3562:\tTraining loss(0.8467898368835449)\n",
      "BATCH 1000/3562:\tTraining loss(0.405559241771698)\n",
      "BATCH 1100/3562:\tTraining loss(0.46286144852638245)\n",
      "BATCH 1200/3562:\tTraining loss(0.5424733757972717)\n",
      "BATCH 1300/3562:\tTraining loss(0.5719776749610901)\n",
      "BATCH 1400/3562:\tTraining loss(0.6917942762374878)\n",
      "BATCH 1500/3562:\tTraining loss(0.725612998008728)\n",
      "BATCH 1600/3562:\tTraining loss(0.43401408195495605)\n",
      "BATCH 1700/3562:\tTraining loss(0.4001852869987488)\n",
      "BATCH 1800/3562:\tTraining loss(0.42826128005981445)\n",
      "BATCH 1900/3562:\tTraining loss(0.30031853914260864)\n",
      "BATCH 2000/3562:\tTraining loss(0.362758606672287)\n",
      "BATCH 2100/3562:\tTraining loss(0.23168598115444183)\n",
      "BATCH 2200/3562:\tTraining loss(0.5697044134140015)\n",
      "BATCH 2300/3562:\tTraining loss(0.41868969798088074)\n",
      "BATCH 2400/3562:\tTraining loss(0.4108319878578186)\n",
      "BATCH 2500/3562:\tTraining loss(0.26256462931632996)\n",
      "BATCH 2600/3562:\tTraining loss(0.49371373653411865)\n",
      "BATCH 2700/3562:\tTraining loss(0.37781625986099243)\n",
      "BATCH 2800/3562:\tTraining loss(0.1868463009595871)\n",
      "BATCH 2900/3562:\tTraining loss(0.2540888488292694)\n",
      "BATCH 3000/3562:\tTraining loss(0.606939971446991)\n",
      "BATCH 3100/3562:\tTraining loss(0.4680944085121155)\n",
      "BATCH 3200/3562:\tTraining loss(0.11627783626317978)\n",
      "BATCH 3300/3562:\tTraining loss(0.6354726552963257)\n",
      "BATCH 3400/3562:\tTraining loss(0.4119250178337097)\n",
      "BATCH 3500/3562:\tTraining loss(0.18780487775802612)\n",
      "\n",
      "Avg training loss:    0.41879136500705466\n",
      "Avg validation loss:  0.1947435947991155\n",
      "F1 validation score:  {'f1': 0.9275726630007856}\n",
      "\n",
      "\n",
      "Avg test loss:  0.30724449477374854\n",
      "F1 test score:  {'f1': 0.8927886742368382}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from model_preparation import set_seed\n",
    "from data_preparation import _load_dataset, _prepare_data, _create_dataloaders, _create_tensors\n",
    "from preprocessor import Preprocessor\n",
    "from helper_functions import get_device\n",
    "from datasets import load_metric\n",
    "import torch\n",
    "import numpy as np\n",
    "from model_preparation import Model, set_seed\n",
    "from helper_functions import get_device\n",
    "from datasets import load_metric\n",
    "\n",
    "\n",
    "# Parameters\n",
    "TRAIN_DATA_PATH = \"../data/train_aug.csv\"\n",
    "TEST_DATA_PATH = \"../data/eval.csv\"\n",
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 1\n",
    "SEED = 42\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "device = get_device()\n",
    "\n",
    "\n",
    "# 'microsoft/deberta-v2-xlarge'\n",
    "# \"bert-base-cased\"\n",
    "# \"bert-base-uncased\"\n",
    "for MODEL_NAME in ['microsoft/deberta-base']:\n",
    "    for pipeline in [[\n",
    "                # 'lowercase',\n",
    "                'hyperlinks',\n",
    "                # 'remove_hyperlinks',\n",
    "                'mentions',\n",
    "                # 'remove_mentions',\n",
    "                'hashtags',\n",
    "                # 'remove_hashtags',\n",
    "                'retweet',\n",
    "                'repetitions',\n",
    "                'emojis',\n",
    "                'smileys',\n",
    "                # 'punctuation',\n",
    "                'spaces',\n",
    "                # 'tokenize'\n",
    "            ],\n",
    "            ]:\n",
    "\n",
    "        def custom_clean_text(text: str) -> str:\n",
    "            preprocessor = Preprocessor(pipeline)\n",
    "            return preprocessor(text)\n",
    "\n",
    "\n",
    "        if TRAIN_DATA_PATH == \"../data/train_aug.csv\":\n",
    "            df = _load_dataset(TRAIN_DATA_PATH)\n",
    "        else:\n",
    "            df = _load_dataset(TRAIN_DATA_PATH)\n",
    "            df = _prepare_data(df, custom_clean_text)\n",
    "\n",
    "        input_ids, attention_masks, labels = _create_tensors(df, MODEL_NAME)\n",
    "        train_dataloader, validation_dataloader = _create_dataloaders(input_ids, attention_masks, labels, BATCH_SIZE, \n",
    "                                                                    create_validation_set= True)\n",
    "\n",
    "        df = _load_dataset(TEST_DATA_PATH)\n",
    "        df = _prepare_data(df, custom_clean_text)\n",
    "        input_ids, attention_masks, labels = _create_tensors(df, MODEL_NAME)\n",
    "        test_dataloader = _create_dataloaders(input_ids, attention_masks, labels, BATCH_SIZE, create_validation_set= False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "        model_class = Model(MODEL_NAME, NUM_EPOCHS, len(train_dataloader))\n",
    "        model, optimizer, lr_scheduler = model_class.get_model_optimizer_scheduler()\n",
    "        model = model.to(device)\n",
    "\n",
    "        training_loss = 0\n",
    "        val_loss = 0\n",
    "        val_f1 = 0\n",
    "\n",
    "        training_stats = []\n",
    "        try:\n",
    "            for epoch in range(NUM_EPOCHS):\n",
    "                print(f\"EPOCH {epoch+1}/{NUM_EPOCHS}\\n\")\n",
    "                model.train()\n",
    "                total_train_loss = 0\n",
    "\n",
    "                for step, batch in enumerate(train_dataloader):\n",
    "                    model.zero_grad()\n",
    "                    parameters = {\n",
    "                        \"input_ids\" : batch[0].to(device),\n",
    "                        \"attention_mask\" :  batch[1].to(device), \n",
    "                        \"labels\" : batch[2].to(device)\n",
    "                    }\n",
    "                    outputs = model(**parameters)\n",
    "\n",
    "                    loss = outputs.loss\n",
    "                    total_train_loss += loss.item()\n",
    "                    loss.backward()\n",
    "\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "                    optimizer.step()\n",
    "                    lr_scheduler.step()\n",
    "                    optimizer.zero_grad()\n",
    "                    # progress_bar.update(1)\n",
    "\n",
    "                    if step % 100 == 0 and step != 0:\n",
    "                        print(f\"BATCH {step}/{len(train_dataloader)}:\\tTraining loss({loss.item()})\")\n",
    "\n",
    "                training_stats.append({\n",
    "                    \"epoch\":epoch+1,\n",
    "                    \"training_loss\":total_train_loss/len(train_dataloader)\n",
    "                    })\n",
    "\n",
    "                total_val_loss = 0\n",
    "                metric = load_metric(\"f1\")\n",
    "\n",
    "                model.eval()\n",
    "                for batch in validation_dataloader:\n",
    "\n",
    "                    parameters = {\n",
    "                        \"input_ids\" : batch[0].to(device),\n",
    "                        \"attention_mask\" :  batch[1].to(device), \n",
    "                        \"labels\" : batch[2].to(device)\n",
    "                    }\n",
    "                    with torch.no_grad():\n",
    "                        outputs = model(**parameters)\n",
    "\n",
    "                    logits = outputs.logits\n",
    "                    loss = outputs.loss\n",
    "                    total_val_loss += loss.item()\n",
    "\n",
    "                    predictions = torch.argmax(logits, dim=-1)\n",
    "                    metric.add_batch(predictions=predictions, references=parameters[\"labels\"])\n",
    "\n",
    "                training_stats[epoch][\"validation_loss\"] = total_val_loss/len(validation_dataloader)\n",
    "                training_stats[epoch][\"validation_f1_score\"] = metric.compute()\n",
    "\n",
    "                print(f\"\\nAvg training loss:    {training_stats[epoch]['training_loss']}\")\n",
    "                print(f\"Avg validation loss:  {training_stats[epoch]['validation_loss']}\")\n",
    "                print(f\"F1 validation score:  {training_stats[epoch]['validation_f1_score']}\\n\")\n",
    "\n",
    "                training_loss = training_stats[epoch]['training_loss']\n",
    "                val_loss = training_stats[epoch]['validation_loss']\n",
    "                val_f1 = training_stats[epoch]['validation_f1_score']\n",
    "\n",
    "        except RuntimeError as e:\n",
    "            print(e)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        model = model.to(device)\n",
    "        testing_stats = []\n",
    "\n",
    "        try:\n",
    "            total_test_loss = 0\n",
    "            metric = load_metric(\"f1\")\n",
    "\n",
    "            model.eval()\n",
    "\n",
    "            for n, batch in enumerate(test_dataloader):\n",
    "\n",
    "                parameters = {\n",
    "                    \"input_ids\" : batch[0].to(device),\n",
    "                    \"attention_mask\" :  batch[1].to(device), \n",
    "                    \"labels\" : batch[2].to(device)\n",
    "                }\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(**parameters)\n",
    "                \n",
    "                logits = outputs.logits\n",
    "                loss = outputs.loss\n",
    "                total_test_loss += loss.item()\n",
    "\n",
    "                predictions = torch.argmax(logits, dim=-1)\n",
    "                metric.add_batch(predictions=predictions, references=parameters[\"labels\"])\n",
    "\n",
    "            testing_stats.append({\n",
    "                \"test_loss\": total_test_loss/len(test_dataloader),\n",
    "                \"test_f1_score\": metric.compute()\n",
    "            })\n",
    "\n",
    "            print(f\"\\nAvg test loss:  {testing_stats[0]['test_loss']}\")\n",
    "            print(f\"F1 test score:  {testing_stats[0]['test_f1_score']}\\n\")\n",
    "\n",
    "\n",
    "            results_dictionary[\"model_name\"].append(MODEL_NAME)\n",
    "            results_dictionary[\"pipeline\"].append(str(pipeline))\n",
    "            results_dictionary[\"training_loss\"].append(training_loss)\n",
    "            results_dictionary[\"validation_loss\"].append(val_loss)\n",
    "            results_dictionary[\"validation_f1\"].append(val_f1)\n",
    "            results_dictionary[\"test_loss\"].append(testing_stats[0]['test_loss'])\n",
    "            results_dictionary[\"test_f1\"].append(testing_stats[0]['test_f1_score'])\n",
    "            if TRAIN_DATA_PATH == \"../data/train_aug.csv\":\n",
    "                results_dictionary[\"augmented\"].append(\"yes\")\n",
    "            else:\n",
    "                results_dictionary[\"augmented\"].append(\"no\")\n",
    "\n",
    "\n",
    "        except RuntimeError as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>pipeline</th>\n",
       "      <th>training_loss</th>\n",
       "      <th>validation_loss</th>\n",
       "      <th>validation_f1</th>\n",
       "      <th>test_loss</th>\n",
       "      <th>test_f1</th>\n",
       "      <th>augmented</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>microsoft/deberta-base</td>\n",
       "      <td>['hyperlinks', 'mentions', 'hashtags', 'retweet', 'repetitions', 'emojis', 'smileys', 'spaces']</td>\n",
       "      <td>0.539779</td>\n",
       "      <td>0.498220</td>\n",
       "      <td>{'f1': 0.7855407047387605}</td>\n",
       "      <td>0.510469</td>\n",
       "      <td>{'f1': 0.7723367697594502}</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>microsoft/deberta-base</td>\n",
       "      <td>['lowercase', 'hyperlinks', 'mentions', 'hashtags', 'retweet', 'repetitions', 'emojis', 'smileys', 'spaces']</td>\n",
       "      <td>0.560275</td>\n",
       "      <td>0.507515</td>\n",
       "      <td>{'f1': 0.7865235539654144}</td>\n",
       "      <td>0.519109</td>\n",
       "      <td>{'f1': 0.7712455516014236}</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>microsoft/deberta-base</td>\n",
       "      <td>['hyperlinks', 'mentions', 'hashtags', 'repetitions', 'emojis', 'smileys', 'spaces']</td>\n",
       "      <td>0.544365</td>\n",
       "      <td>0.501752</td>\n",
       "      <td>{'f1': 0.7833935018050541}</td>\n",
       "      <td>0.512311</td>\n",
       "      <td>{'f1': 0.7747542384955122}</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>microsoft/deberta-base</td>\n",
       "      <td>['hyperlinks', 'mentions', 'hashtags', 'retweet', 'emojis', 'smileys', 'spaces']</td>\n",
       "      <td>0.555020</td>\n",
       "      <td>0.518818</td>\n",
       "      <td>{'f1': 0.7694189602446483}</td>\n",
       "      <td>0.504441</td>\n",
       "      <td>{'f1': 0.777521613832853}</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>microsoft/deberta-base</td>\n",
       "      <td>['hyperlinks', 'mentions', 'hashtags', 'retweet', 'repetitions', 'emojis', 'smileys', 'spaces']</td>\n",
       "      <td>0.418791</td>\n",
       "      <td>0.194744</td>\n",
       "      <td>{'f1': 0.9275726630007856}</td>\n",
       "      <td>0.307244</td>\n",
       "      <td>{'f1': 0.8927886742368382}</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               model_name  \\\n",
       "0  microsoft/deberta-base   \n",
       "1  microsoft/deberta-base   \n",
       "2  microsoft/deberta-base   \n",
       "3  microsoft/deberta-base   \n",
       "4  microsoft/deberta-base   \n",
       "\n",
       "                                                                                                       pipeline  \\\n",
       "0               ['hyperlinks', 'mentions', 'hashtags', 'retweet', 'repetitions', 'emojis', 'smileys', 'spaces']   \n",
       "1  ['lowercase', 'hyperlinks', 'mentions', 'hashtags', 'retweet', 'repetitions', 'emojis', 'smileys', 'spaces']   \n",
       "2                          ['hyperlinks', 'mentions', 'hashtags', 'repetitions', 'emojis', 'smileys', 'spaces']   \n",
       "3                              ['hyperlinks', 'mentions', 'hashtags', 'retweet', 'emojis', 'smileys', 'spaces']   \n",
       "4               ['hyperlinks', 'mentions', 'hashtags', 'retweet', 'repetitions', 'emojis', 'smileys', 'spaces']   \n",
       "\n",
       "   training_loss  validation_loss               validation_f1  test_loss  \\\n",
       "0       0.539779         0.498220  {'f1': 0.7855407047387605}   0.510469   \n",
       "1       0.560275         0.507515  {'f1': 0.7865235539654144}   0.519109   \n",
       "2       0.544365         0.501752  {'f1': 0.7833935018050541}   0.512311   \n",
       "3       0.555020         0.518818  {'f1': 0.7694189602446483}   0.504441   \n",
       "4       0.418791         0.194744  {'f1': 0.9275726630007856}   0.307244   \n",
       "\n",
       "                      test_f1 augmented  \n",
       "0  {'f1': 0.7723367697594502}        no  \n",
       "1  {'f1': 0.7712455516014236}        no  \n",
       "2  {'f1': 0.7747542384955122}        no  \n",
       "3   {'f1': 0.777521613832853}        no  \n",
       "4  {'f1': 0.8927886742368382}       yes  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "results_df = pd.DataFrame(results_dictionary)\n",
    "results_df.to_csv(\"2nd_models_comparison.csv\", index=False)\n",
    "results_df.head(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataframe preprocessing steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Selsy mistakenly thinks the #Brownlow is like Mad Monday and wears costume. #SelwoodJoelSelwood http://t.co/H3Prjyjlyj</td>\n",
       "      <td>Selsy mistakenly thinks the Brownlow is like Mad Monday and wears costume. SelwoodJoelSelwood url</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Obit vs mesin</td>\n",
       "      <td>Obit vs mesin</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>growing watercolor - Decatur Cemetery This painting was done twice, the first time did not turn out well at... http://t.co/YiRp9mEK2y</td>\n",
       "      <td>growing watercolor - Decatur Cemetery This painting was done twice, the first time did not turn out well at.. url</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>your new chick that's my ex bitch homie in fact I'm still logged into her Netflix homie</td>\n",
       "      <td>your new chick that's my ex bitch homie in fact I'm still logged into her Netflix homie</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://t.co/tTYrceHxzg @nationaljournal &amp;amp;Is Hillary Clinton Too Hawkish for Iowa Democrats? http://t.co/pbEZJ0obmK</td>\n",
       "      <td>url mention &amp;amp;Is Hillary Clinton Too Hawkish for Iowa Democrats? url</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>They sleepin on you</td>\n",
       "      <td>They sleepin on you</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Uh oh! Chris Weidman is ducking Vitor Belfort hahahahahaha but I don't blame him 😅</td>\n",
       "      <td>Uh oh! Chris Weidman is ducking Vitor Belfort hahahahahaha but I don't blame him emoji</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>RT @andihfg: \"remember when i said hahmkyul are the only normal ones in the group? ... http://t.co/adioIdSjq4\" lee qri? normal? http://t.co…</td>\n",
       "      <td>retweet mention : \"remember when i said hahmkyul are the only normal ones in the group? .. url lee qri? normal? url</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>\"@LosMyTeddyBear: @TheCarlosPena\\n#LosMeetMilou\\nI need you please! \\nWatch my videos+tweets ♡\\n:\"( \\n73\"</td>\n",
       "      <td>\" mention : mention LosMeetMilou I need you please! Watch my videos+tweets ♡ smiley 73\"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>RT @trvpvv: @OsamaJames lol because its clear that they take pride in taking pictures like this and exploiting females</td>\n",
       "      <td>retweet mention : mention lol because its clear that they take pride in taking pictures like this and exploiting females</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>RT @RichardBarrow: Correspondent from Telegraph @tomphillipsin: Just got to Koh Tao. Papers say island 'sealed off'. Not true http://t.co/k…</td>\n",
       "      <td>retweet mention : Correspondent from Telegraph mention : Just got to Koh Tao. Papers say island 'sealed off'. Not true url</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>RT @SarahDJakes: If You're constantly asking God for something You never have time to ask Him what He needs from you.</td>\n",
       "      <td>retweet mention : If You're constantly asking God for something You never have time to ask Him what He needs from you.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Are you ready today?haha xD zenam zenam..yuhuuu.. http://t.co/QpfdEofNnx</td>\n",
       "      <td>Are you ready today?haha smiley zenam zenam..yuhuu.. url</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>@MFJosh__ necessary</td>\n",
       "      <td>mention necessary</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Danica McKellar says she's 'the happiest I've ever been,' which makes us really, really happy http://t.co/IAtK83m63b</td>\n",
       "      <td>Danica McKellar says she's 'the happiest I've ever been,' which makes us really, really happy url</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Aye @onlyfrustrated thats how you score boy!! http://t.co/eggZTUILj0</td>\n",
       "      <td>Aye mention thats how you score boy!! url</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>THAT BUDWEISER COMMERCIAL.  GOODBYE.</td>\n",
       "      <td>THAT BUDWEISER COMMERCIAL. GOODBYE.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>RT @AZSports: Who wants to watch the #AZCardinals make it 3-0 this weekend  with a win against the #Niners? RT to win 2 tickets.  #AZSports…</td>\n",
       "      <td>retweet mention : Who wants to watch the AZCardinals make it 3-0 this weekend with a win against the Niners? retweet to win 2 tickets. AZSports…</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>His dancing ass lol</td>\n",
       "      <td>His dancing ass lol</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>I Got that fire u're searching for! rappers check out my beats!\\n↪🌟http://t.co/q74FHNWcnM🌟\\n#sendbeats #needbeats #hiphop</td>\n",
       "      <td>I Got that fire u're searching for! rappers check out my beats! url sendbeats needbeats hiphop</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>I think tomorrow I shall be nonexistent 👌</td>\n",
       "      <td>I think tomorrow I shall be nonexistent emoji</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>RT @thatkillahh_: I love reggae music</td>\n",
       "      <td>retweet mention : I love reggae music</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>RT @ESPNCFB: Amari Cooper has now had six-straight 100-yard games.\\n\\nThat is the most by an SEC WR over the last decade. #UFvsBAMA http://t.…</td>\n",
       "      <td>retweet mention : Amari Cooper has now had six-straight 100-yard games. That is the most by an SEC WR over the last decade. UFvsBAMA url</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>OMG THEY'RE AREN'T A MONOLITH.</td>\n",
       "      <td>OMG THEY'RE AREN'T A MONOLITH.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>RT @MovieCrow: Video: #BangBang Title track song teaser http://t.co/8YgS5GN3Vv @iHrithikFan @HrItHiK_FaNcLuB @KatrinaKaifTeam http://t.co/t…</td>\n",
       "      <td>retweet mention : Video: BangBang Title track song teaser url mention mention mention url</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Y'all are some creeps for real..</td>\n",
       "      <td>Y'all are some creeps for real..</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>RT @ianftpaul: Friendly reminder that these are the boys who wrote 18\\n\\n*she's got a naughty tattoo in a place that i wanna get to* http://t…</td>\n",
       "      <td>retweet mention : Friendly reminder that these are the boys who wrote 18 *she's got a naughty tattoo in a place that i wanna get to* url</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>I should sleep, because when that alarm goes off at 5am ☹ &amp;lt;/3</td>\n",
       "      <td>I should sleep, because when that alarm goes off at 5am ☹ &amp;lt;/3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>RT @LeeBlakeman: Port #Vale manager Micky Adams has left the club</td>\n",
       "      <td>retweet mention : Port Vale manager Micky Adams has left the club</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>I Aint Worried Bout Bo Hoe 😩🙅 Cos I Switch Them Like I Switch Clothes 😜💁👯👯😂💯</td>\n",
       "      <td>I Aint Worried Bout Bo Hoe emoji Cos I Switch Them Like I Switch Clothes emoji</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Here's The Best Explanation We've Heard Of How Scotland Could Still Shock The World And Vote YES http://t.co/FVDRMENdFg via @themoneygame</td>\n",
       "      <td>Here's The Best Explanation We've Heard Of How Scotland Could Still Shock The World And Vote YES url via mention</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>RT @ArianaGrande: the happiest</td>\n",
       "      <td>retweet mention : the happiest</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>\"Kimberlaylay\"\\n*Makes weird, confused face*</td>\n",
       "      <td>\"Kimberlaylay\" *Makes weird, confused face*</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>RT @SML_trey: Late night shooting session wit the shooter @_writewithlove http://t.co/JwPuSQsHMM</td>\n",
       "      <td>retweet mention : Late night shooting session wit the shooter mention url</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Off to school...maybe I can come back to @Luke5SOS following me?</td>\n",
       "      <td>Off to school..maybe I can come back to mention following me?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>RT @JeezyJake10: LOL GPhi yes 👌👏😂</td>\n",
       "      <td>retweet mention : LOL GPhi yes emoji</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Turn to Channel 4 to catch the NFL, but it's the end of Sixth Sense, where Bruce Willis is actually Mickey Rourke's career all along #burn</td>\n",
       "      <td>Turn to Channel 4 to catch the NFL, but it's the end of Sixth Sense, where Bruce Willis is actually Mickey Rourke's career all along burn</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>RT @pleasuremenarry: WHAT IS WITH LIAMS ARM??? http://t.co/thcQYEpDAT</td>\n",
       "      <td>retweet mention : WHAT IS WITH LIAMS ARM?? url</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>I believe in Karma what you give is what you get returned.</td>\n",
       "      <td>I believe in Karma what you give is what you get returned.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>RT thorn_alan: #gamedev Absolute Perfectionism involves infinite recursion. To release your game, you'll need to stop somewhere</td>\n",
       "      <td>retweet thorn_alan: gamedev Absolute Perfectionism involves infinite recursion. To release your game, you'll need to stop somewhere</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                             tweet  \\\n",
       "0                           Selsy mistakenly thinks the #Brownlow is like Mad Monday and wears costume. #SelwoodJoelSelwood http://t.co/H3Prjyjlyj   \n",
       "1                                                                                                                                    Obit vs mesin   \n",
       "2            growing watercolor - Decatur Cemetery This painting was done twice, the first time did not turn out well at... http://t.co/YiRp9mEK2y   \n",
       "3                                                          your new chick that's my ex bitch homie in fact I'm still logged into her Netflix homie   \n",
       "4                           http://t.co/tTYrceHxzg @nationaljournal &amp;Is Hillary Clinton Too Hawkish for Iowa Democrats? http://t.co/pbEZJ0obmK   \n",
       "5                                                                                                                              They sleepin on you   \n",
       "6                                                               Uh oh! Chris Weidman is ducking Vitor Belfort hahahahahaha but I don't blame him 😅   \n",
       "7     RT @andihfg: \"remember when i said hahmkyul are the only normal ones in the group? ... http://t.co/adioIdSjq4\" lee qri? normal? http://t.co…   \n",
       "8                                        \"@LosMyTeddyBear: @TheCarlosPena\\n#LosMeetMilou\\nI need you please! \\nWatch my videos+tweets ♡\\n:\"( \\n73\"   \n",
       "9                           RT @trvpvv: @OsamaJames lol because its clear that they take pride in taking pictures like this and exploiting females   \n",
       "10    RT @RichardBarrow: Correspondent from Telegraph @tomphillipsin: Just got to Koh Tao. Papers say island 'sealed off'. Not true http://t.co/k…   \n",
       "11                           RT @SarahDJakes: If You're constantly asking God for something You never have time to ask Him what He needs from you.   \n",
       "12                                                                        Are you ready today?haha xD zenam zenam..yuhuuu.. http://t.co/QpfdEofNnx   \n",
       "13                                                                                                                             @MFJosh__ necessary   \n",
       "14                            Danica McKellar says she's 'the happiest I've ever been,' which makes us really, really happy http://t.co/IAtK83m63b   \n",
       "15                                                                            Aye @onlyfrustrated thats how you score boy!! http://t.co/eggZTUILj0   \n",
       "16                                                                                                            THAT BUDWEISER COMMERCIAL.  GOODBYE.   \n",
       "17    RT @AZSports: Who wants to watch the #AZCardinals make it 3-0 this weekend  with a win against the #Niners? RT to win 2 tickets.  #AZSports…   \n",
       "18                                                                                                                             His dancing ass lol   \n",
       "19                       I Got that fire u're searching for! rappers check out my beats!\\n↪🌟http://t.co/q74FHNWcnM🌟\\n#sendbeats #needbeats #hiphop   \n",
       "20                                                                                                       I think tomorrow I shall be nonexistent 👌   \n",
       "21                                                                                                           RT @thatkillahh_: I love reggae music   \n",
       "22  RT @ESPNCFB: Amari Cooper has now had six-straight 100-yard games.\\n\\nThat is the most by an SEC WR over the last decade. #UFvsBAMA http://t.…   \n",
       "23                                                                                                                  OMG THEY'RE AREN'T A MONOLITH.   \n",
       "24    RT @MovieCrow: Video: #BangBang Title track song teaser http://t.co/8YgS5GN3Vv @iHrithikFan @HrItHiK_FaNcLuB @KatrinaKaifTeam http://t.co/t…   \n",
       "25                                                                                                                Y'all are some creeps for real..   \n",
       "26  RT @ianftpaul: Friendly reminder that these are the boys who wrote 18\\n\\n*she's got a naughty tattoo in a place that i wanna get to* http://t…   \n",
       "27                                                                                I should sleep, because when that alarm goes off at 5am ☹ &lt;/3   \n",
       "28                                                                               RT @LeeBlakeman: Port #Vale manager Micky Adams has left the club   \n",
       "29                                                                    I Aint Worried Bout Bo Hoe 😩🙅 Cos I Switch Them Like I Switch Clothes 😜💁👯👯😂💯   \n",
       "30       Here's The Best Explanation We've Heard Of How Scotland Could Still Shock The World And Vote YES http://t.co/FVDRMENdFg via @themoneygame   \n",
       "31                                                                                                                  RT @ArianaGrande: the happiest   \n",
       "32                                                                                                    \"Kimberlaylay\"\\n*Makes weird, confused face*   \n",
       "33                                                RT @SML_trey: Late night shooting session wit the shooter @_writewithlove http://t.co/JwPuSQsHMM   \n",
       "34                                                                                Off to school...maybe I can come back to @Luke5SOS following me?   \n",
       "35                                                                                                               RT @JeezyJake10: LOL GPhi yes 👌👏😂   \n",
       "36      Turn to Channel 4 to catch the NFL, but it's the end of Sixth Sense, where Bruce Willis is actually Mickey Rourke's career all along #burn   \n",
       "37                                                                           RT @pleasuremenarry: WHAT IS WITH LIAMS ARM??? http://t.co/thcQYEpDAT   \n",
       "38                                                                                      I believe in Karma what you give is what you get returned.   \n",
       "39                 RT thorn_alan: #gamedev Absolute Perfectionism involves infinite recursion. To release your game, you'll need to stop somewhere   \n",
       "\n",
       "                                                                                                                                                 text  \\\n",
       "0                                                  Selsy mistakenly thinks the Brownlow is like Mad Monday and wears costume. SelwoodJoelSelwood url    \n",
       "1                                                                                                                                       Obit vs mesin   \n",
       "2                                  growing watercolor - Decatur Cemetery This painting was done twice, the first time did not turn out well at.. url    \n",
       "3                                                             your new chick that's my ex bitch homie in fact I'm still logged into her Netflix homie   \n",
       "4                                                                            url mention &amp;Is Hillary Clinton Too Hawkish for Iowa Democrats? url    \n",
       "5                                                                                                                                 They sleepin on you   \n",
       "6                                                             Uh oh! Chris Weidman is ducking Vitor Belfort hahahahahaha but I don't blame him emoji    \n",
       "7                                retweet mention : \"remember when i said hahmkyul are the only normal ones in the group? .. url lee qri? normal? url    \n",
       "8                                                             \" mention : mention LosMeetMilou I need you please! Watch my videos+tweets ♡ smiley 73\"   \n",
       "9                            retweet mention : mention lol because its clear that they take pride in taking pictures like this and exploiting females   \n",
       "10                        retweet mention : Correspondent from Telegraph mention : Just got to Koh Tao. Papers say island 'sealed off'. Not true url    \n",
       "11                             retweet mention : If You're constantly asking God for something You never have time to ask Him what He needs from you.   \n",
       "12                                                                                          Are you ready today?haha smiley zenam zenam..yuhuu.. url    \n",
       "13                                                                                                                                  mention necessary   \n",
       "14                                                 Danica McKellar says she's 'the happiest I've ever been,' which makes us really, really happy url    \n",
       "15                                                                                                         Aye mention thats how you score boy!! url    \n",
       "16                                                                                                                THAT BUDWEISER COMMERCIAL. GOODBYE.   \n",
       "17  retweet mention : Who wants to watch the AZCardinals make it 3-0 this weekend with a win against the Niners? retweet to win 2 tickets. AZSports…    \n",
       "18                                                                                                                                His dancing ass lol   \n",
       "19                                                    I Got that fire u're searching for! rappers check out my beats! url sendbeats needbeats hiphop    \n",
       "20                                                                                                     I think tomorrow I shall be nonexistent emoji    \n",
       "21                                                                                                              retweet mention : I love reggae music   \n",
       "22          retweet mention : Amari Cooper has now had six-straight 100-yard games. That is the most by an SEC WR over the last decade. UFvsBAMA url    \n",
       "23                                                                                                                     OMG THEY'RE AREN'T A MONOLITH.   \n",
       "24                                                         retweet mention : Video: BangBang Title track song teaser url mention mention mention url    \n",
       "25                                                                                                                   Y'all are some creeps for real..   \n",
       "26          retweet mention : Friendly reminder that these are the boys who wrote 18 *she's got a naughty tattoo in a place that i wanna get to* url    \n",
       "27                                                                                   I should sleep, because when that alarm goes off at 5am ☹ &lt;/3   \n",
       "28                                                                                  retweet mention : Port Vale manager Micky Adams has left the club   \n",
       "29                                                                    I Aint Worried Bout Bo Hoe emoji Cos I Switch Them Like I Switch Clothes emoji    \n",
       "30                                  Here's The Best Explanation We've Heard Of How Scotland Could Still Shock The World And Vote YES url via mention    \n",
       "31                                                                                                                     retweet mention : the happiest   \n",
       "32                                                                                                        \"Kimberlaylay\" *Makes weird, confused face*   \n",
       "33                                                                         retweet mention : Late night shooting session wit the shooter mention url    \n",
       "34                                                                                      Off to school..maybe I can come back to mention following me?   \n",
       "35                                                                                                              retweet mention : LOL GPhi yes emoji    \n",
       "36         Turn to Channel 4 to catch the NFL, but it's the end of Sixth Sense, where Bruce Willis is actually Mickey Rourke's career all along burn    \n",
       "37                                                                                                    retweet mention : WHAT IS WITH LIAMS ARM?? url    \n",
       "38                                                                                         I believe in Karma what you give is what you get returned.   \n",
       "39                retweet thorn_alan: gamedev Absolute Perfectionism involves infinite recursion. To release your game, you'll need to stop somewhere   \n",
       "\n",
       "   label  \n",
       "0      0  \n",
       "1      0  \n",
       "2      0  \n",
       "3      1  \n",
       "4      0  \n",
       "5      0  \n",
       "6      0  \n",
       "7      1  \n",
       "8      0  \n",
       "9      1  \n",
       "10     1  \n",
       "11     0  \n",
       "12     1  \n",
       "13     0  \n",
       "14     1  \n",
       "15     1  \n",
       "16     0  \n",
       "17     0  \n",
       "18     0  \n",
       "19     0  \n",
       "20     1  \n",
       "21     1  \n",
       "22     0  \n",
       "23     1  \n",
       "24     0  \n",
       "25     1  \n",
       "26     1  \n",
       "27     1  \n",
       "28     0  \n",
       "29     1  \n",
       "30     0  \n",
       "31     1  \n",
       "32     1  \n",
       "33     0  \n",
       "34     0  \n",
       "35     1  \n",
       "36     0  \n",
       "37     0  \n",
       "38     0  \n",
       "39     0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from model_preparation import set_seed\n",
    "from data_preparation import _load_dataset, _prepare_data, _create_dataloaders, _create_tensors\n",
    "from preprocessor import Preprocessor\n",
    "\n",
    "\n",
    "data_path = \"../data/eval.csv\"\n",
    "model_name = 'microsoft/deberta-base'\n",
    "batch_size = 16\n",
    "create_validation_set = False\n",
    "SEED = 42\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "\n",
    "\n",
    "def custom_clean_text(text: str) -> str:\n",
    "\n",
    "    pipeline = ['hyperlinks', 'mentions', 'hashtags', 'retweet', 'repetitions', 'emojis', 'smileys', 'spaces']\n",
    "\n",
    "    preprocessor = Preprocessor(pipeline)\n",
    "\n",
    "    return preprocessor(text)\n",
    "\n",
    "\n",
    "df = _load_dataset(data_path)\n",
    "\n",
    "final_df = df[\"tweet\"].copy()\n",
    "\n",
    "df = _prepare_data(df, custom_clean_text)\n",
    "\n",
    "final_df = pd.concat([final_df, df[[\"text\",\"label\"]]], axis = 1)\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "final_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check error cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions import get_device\n",
    "from datasets import load_metric\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "input_ids, attention_masks, labels = _create_tensors(df, model_name)\n",
    "dataloaders = _create_dataloaders(input_ids, attention_masks, labels, batch_size, create_validation_set)\n",
    "\n",
    "\n",
    "device = get_device()\n",
    "model = model.to(device)\n",
    "\n",
    "pred_list = np.array([])\n",
    "\n",
    "test_dataloader = dataloaders\n",
    "\n",
    "testing_stats = []\n",
    "\n",
    "try:\n",
    "    total_test_loss = 0\n",
    "    metric = load_metric(\"f1\")\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    for n, batch in enumerate(test_dataloader):\n",
    "\n",
    "        parameters = {\n",
    "            \"input_ids\" : batch[0].to(device),\n",
    "            \"attention_mask\" :  batch[1].to(device), \n",
    "            \"labels\" : batch[2].to(device)\n",
    "        }\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**parameters)\n",
    "        \n",
    "        logits = outputs.logits\n",
    "        loss = outputs.loss\n",
    "        total_test_loss += loss.item()\n",
    "\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        metric.add_batch(predictions=predictions, references=parameters[\"labels\"])\n",
    "\n",
    "        if input_ids[n*32:(n+1)*32].equal(parameters[\"input_ids\"].cpu()):\n",
    "            pred_list = np.append(pred_list,predictions.cpu().numpy())\n",
    "\n",
    "    testing_stats.append({\n",
    "        \"test_loss\": total_test_loss/len(test_dataloader),\n",
    "        \"test_f1_score\": metric.compute()\n",
    "    })\n",
    "\n",
    "    print(f\"\\nAvg test loss:  {testing_stats[0]['test_loss']}\")\n",
    "    print(f\"F1 test score:  {testing_stats[0]['test_f1_score']}\\n\")\n",
    "\n",
    "except RuntimeError as e:\n",
    "    print(e)\n",
    "\n",
    "final_df[\"prediction\"] = pred_list\n",
    "final_df[\"match\"] = final_df.apply(lambda row: \"\" if row[\"label\"] == row[\"prediction\"] else \"NO\", axis=1)\n",
    "\n",
    "# final_df.to_csv(\"predictions.csv\",index=False)\n",
    "final_df[final_df[\"match\"]==\"NO\"].head(40)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a97517a1f2e44bae43a0d1adced36113c0aa0bace4a2569b7d21fd338b4eff8a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
