{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference: https://mccormickml.com/2019/07/22/BERT-fine-tuning/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual training and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import train_model_on_train_data\n",
    "\n",
    "\n",
    "# Parameters\n",
    "TRAIN_DATA_PATH = \"../data/train.csv\"\n",
    "MODEL_NAME = 'microsoft/deberta-base'\n",
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 1\n",
    "\n",
    "model, training_stats = train_model_on_train_data(TRAIN_DATA_PATH, MODEL_NAME, BATCH_SIZE, NUM_EPOCHS)\n",
    "\n",
    "training_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "df_training = pd.DataFrame(training_stats)\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.set(style='darkgrid')\n",
    "sns.set(font_scale=1.5)\n",
    "\n",
    "plt.plot(df_training['training_loss'], 'b-o', label=\"Training\")\n",
    "plt.plot(df_training['validation_loss'], 'g-o', label=\"Validation\")\n",
    "\n",
    "plt.title(\"Training & Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.xticks([1, 2, 3, 4])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import evaluate_on_test_data\n",
    "\n",
    "\n",
    "TEST_DATA_PATH = \"../data/eval.csv\"\n",
    "\n",
    "\n",
    "testing_stats = evaluate_on_test_data(model, TEST_DATA_PATH, MODEL_NAME, BATCH_SIZE)\n",
    "\n",
    "testing_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving Model F1 Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load results_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>pipeline</th>\n",
       "      <th>training_loss</th>\n",
       "      <th>validation_loss</th>\n",
       "      <th>validation_f1</th>\n",
       "      <th>test_loss</th>\n",
       "      <th>test_f1</th>\n",
       "      <th>augmented</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>microsoft/deberta-base</td>\n",
       "      <td>['hyperlinks', 'mentions', 'hashtags', 'retweet', 'repetitions', 'emojis', 'smileys', 'spaces']</td>\n",
       "      <td>0.539779</td>\n",
       "      <td>0.498220</td>\n",
       "      <td>{'f1': 0.7855407047387605}</td>\n",
       "      <td>0.510469</td>\n",
       "      <td>{'f1': 0.7723367697594502}</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>microsoft/deberta-base</td>\n",
       "      <td>['lowercase', 'hyperlinks', 'mentions', 'hashtags', 'retweet', 'repetitions', 'emojis', 'smileys', 'spaces']</td>\n",
       "      <td>0.560275</td>\n",
       "      <td>0.507515</td>\n",
       "      <td>{'f1': 0.7865235539654144}</td>\n",
       "      <td>0.519109</td>\n",
       "      <td>{'f1': 0.7712455516014236}</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>microsoft/deberta-base</td>\n",
       "      <td>['hyperlinks', 'mentions', 'hashtags', 'repetitions', 'emojis', 'smileys', 'spaces']</td>\n",
       "      <td>0.544365</td>\n",
       "      <td>0.501752</td>\n",
       "      <td>{'f1': 0.7833935018050541}</td>\n",
       "      <td>0.512311</td>\n",
       "      <td>{'f1': 0.7747542384955122}</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>microsoft/deberta-base</td>\n",
       "      <td>['hyperlinks', 'mentions', 'hashtags', 'retweet', 'emojis', 'smileys', 'spaces']</td>\n",
       "      <td>0.555020</td>\n",
       "      <td>0.518818</td>\n",
       "      <td>{'f1': 0.7694189602446483}</td>\n",
       "      <td>0.504441</td>\n",
       "      <td>{'f1': 0.777521613832853}</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>microsoft/deberta-base</td>\n",
       "      <td>['hyperlinks', 'mentions', 'hashtags', 'retweet', 'repetitions', 'emojis', 'smileys', 'spaces']</td>\n",
       "      <td>0.418791</td>\n",
       "      <td>0.194744</td>\n",
       "      <td>{'f1': 0.9275726630007856}</td>\n",
       "      <td>0.307244</td>\n",
       "      <td>{'f1': 0.8927886742368382}</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               model_name  \\\n",
       "0  microsoft/deberta-base   \n",
       "1  microsoft/deberta-base   \n",
       "2  microsoft/deberta-base   \n",
       "3  microsoft/deberta-base   \n",
       "4  microsoft/deberta-base   \n",
       "\n",
       "                                                                                                       pipeline  \\\n",
       "0               ['hyperlinks', 'mentions', 'hashtags', 'retweet', 'repetitions', 'emojis', 'smileys', 'spaces']   \n",
       "1  ['lowercase', 'hyperlinks', 'mentions', 'hashtags', 'retweet', 'repetitions', 'emojis', 'smileys', 'spaces']   \n",
       "2                          ['hyperlinks', 'mentions', 'hashtags', 'repetitions', 'emojis', 'smileys', 'spaces']   \n",
       "3                              ['hyperlinks', 'mentions', 'hashtags', 'retweet', 'emojis', 'smileys', 'spaces']   \n",
       "4               ['hyperlinks', 'mentions', 'hashtags', 'retweet', 'repetitions', 'emojis', 'smileys', 'spaces']   \n",
       "\n",
       "   training_loss  validation_loss               validation_f1  test_loss  \\\n",
       "0       0.539779         0.498220  {'f1': 0.7855407047387605}   0.510469   \n",
       "1       0.560275         0.507515  {'f1': 0.7865235539654144}   0.519109   \n",
       "2       0.544365         0.501752  {'f1': 0.7833935018050541}   0.512311   \n",
       "3       0.555020         0.518818  {'f1': 0.7694189602446483}   0.504441   \n",
       "4       0.418791         0.194744  {'f1': 0.9275726630007856}   0.307244   \n",
       "\n",
       "                      test_f1 augmented  \n",
       "0  {'f1': 0.7723367697594502}        no  \n",
       "1  {'f1': 0.7712455516014236}        no  \n",
       "2  {'f1': 0.7747542384955122}        no  \n",
       "3   {'f1': 0.777521613832853}        no  \n",
       "4  {'f1': 0.8927886742368382}       yes  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "results_dictionary = {\n",
    "        \"model_name\": [],\n",
    "        \"pipeline\": [],\n",
    "        \"training_loss\": [],\n",
    "        \"validation_loss\": [],\n",
    "        \"validation_f1\": [],\n",
    "        \"test_loss\": [],\n",
    "        \"test_f1\": [],\n",
    "        \"augmented\": []\n",
    "    }\n",
    "\n",
    "\n",
    "old_df = pd.read_csv(\"../results/2nd_models_comparison.csv\")\n",
    "\n",
    "for i in range(len(old_df)):\n",
    "    aa = old_df.iloc[i].to_dict()\n",
    "\n",
    "    for k,v in aa.items():\n",
    "        results_dictionary[k].append(v)\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "results_df = pd.DataFrame(results_dictionary)\n",
    "results_df.head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dictionary = {\n",
    "        \"model_name\": [],\n",
    "        \"pipeline\": [],\n",
    "        \"training_loss\": [],\n",
    "        \"validation_loss\": [],\n",
    "        \"validation_f1\": [],\n",
    "        \"test_loss\": [],\n",
    "        \"test_f1\": []\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: NVIDIA GeForce RTX 2070 with Max-Q Design\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1/1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lisandro\\Desktop\\sentiment-challenge\\venv\\lib\\site-packages\\transformers\\models\\deberta\\modeling_deberta.py:1207: UserWarning: This overload of nonzero is deprecated:\n",
      "\tnonzero()\n",
      "Consider using one of the following signatures instead:\n",
      "\tnonzero(*, bool as_tuple) (Triggered internally at  ..\\torch\\csrc\\utils\\python_arg_parser.cpp:882.)\n",
      "  label_index = (labels >= 0).nonzero()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BATCH 100/16618:\tTraining loss(0.6907029747962952)\n",
      "BATCH 200/16618:\tTraining loss(0.6671852469444275)\n",
      "BATCH 300/16618:\tTraining loss(0.6304215788841248)\n",
      "BATCH 400/16618:\tTraining loss(0.7610862851142883)\n",
      "BATCH 500/16618:\tTraining loss(0.635147213935852)\n",
      "BATCH 600/16618:\tTraining loss(0.46992871165275574)\n",
      "BATCH 700/16618:\tTraining loss(0.7826963663101196)\n",
      "BATCH 800/16618:\tTraining loss(0.7746791243553162)\n",
      "BATCH 900/16618:\tTraining loss(0.7833300232887268)\n",
      "BATCH 1000/16618:\tTraining loss(0.40574854612350464)\n",
      "BATCH 1100/16618:\tTraining loss(0.6994309425354004)\n",
      "BATCH 1200/16618:\tTraining loss(0.6277515888214111)\n",
      "BATCH 1300/16618:\tTraining loss(0.7659056186676025)\n",
      "BATCH 1400/16618:\tTraining loss(0.3864557445049286)\n",
      "BATCH 1500/16618:\tTraining loss(0.6288545727729797)\n",
      "BATCH 1600/16618:\tTraining loss(0.7589755058288574)\n",
      "BATCH 1700/16618:\tTraining loss(0.44736170768737793)\n",
      "BATCH 1800/16618:\tTraining loss(0.48267626762390137)\n",
      "BATCH 1900/16618:\tTraining loss(0.36919063329696655)\n",
      "BATCH 2000/16618:\tTraining loss(0.5745483040809631)\n",
      "BATCH 2100/16618:\tTraining loss(0.5413281917572021)\n",
      "BATCH 2200/16618:\tTraining loss(0.3909282982349396)\n",
      "BATCH 2300/16618:\tTraining loss(0.5489269495010376)\n",
      "BATCH 2400/16618:\tTraining loss(0.6491953134536743)\n",
      "BATCH 2500/16618:\tTraining loss(0.5520837903022766)\n",
      "BATCH 2600/16618:\tTraining loss(0.5648152828216553)\n",
      "BATCH 2700/16618:\tTraining loss(0.3446366488933563)\n",
      "BATCH 2800/16618:\tTraining loss(0.6041296720504761)\n",
      "BATCH 2900/16618:\tTraining loss(0.5716970562934875)\n",
      "BATCH 3000/16618:\tTraining loss(0.3315710127353668)\n",
      "BATCH 3100/16618:\tTraining loss(0.6178841590881348)\n",
      "BATCH 3200/16618:\tTraining loss(0.25158026814460754)\n",
      "BATCH 3300/16618:\tTraining loss(0.5270804762840271)\n",
      "BATCH 3400/16618:\tTraining loss(0.36446207761764526)\n",
      "BATCH 3500/16618:\tTraining loss(0.7669157981872559)\n",
      "BATCH 3600/16618:\tTraining loss(0.5098058581352234)\n",
      "BATCH 3700/16618:\tTraining loss(0.48189786076545715)\n",
      "BATCH 3800/16618:\tTraining loss(0.41039496660232544)\n",
      "BATCH 3900/16618:\tTraining loss(0.6714243292808533)\n",
      "BATCH 4000/16618:\tTraining loss(0.40169721841812134)\n",
      "BATCH 4100/16618:\tTraining loss(0.44466468691825867)\n",
      "BATCH 4200/16618:\tTraining loss(0.46722057461738586)\n",
      "BATCH 4300/16618:\tTraining loss(0.34503501653671265)\n",
      "BATCH 4400/16618:\tTraining loss(0.5044506192207336)\n",
      "BATCH 4500/16618:\tTraining loss(0.585249125957489)\n",
      "BATCH 4600/16618:\tTraining loss(0.2091376930475235)\n",
      "BATCH 4700/16618:\tTraining loss(0.5844369530677795)\n",
      "BATCH 4800/16618:\tTraining loss(0.26779818534851074)\n",
      "BATCH 4900/16618:\tTraining loss(0.22733578085899353)\n",
      "BATCH 5000/16618:\tTraining loss(0.46325910091400146)\n",
      "BATCH 5100/16618:\tTraining loss(0.42220014333724976)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from model_preparation import set_seed\n",
    "from data_preparation import _load_dataset, _prepare_data, _create_dataloaders, _create_tensors\n",
    "from helper_functions import get_device\n",
    "from datasets import load_metric\n",
    "import torch\n",
    "import numpy as np\n",
    "from model_preparation import Model, set_seed\n",
    "from helper_functions import get_device\n",
    "from datasets import load_metric\n",
    "\n",
    "\n",
    "# Parameters\n",
    "TRAIN_DATA_PATH = \"../data/train_aug.csv\"\n",
    "TEST_DATA_PATH = \"../data/eval.csv\"\n",
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 1\n",
    "SEED = 42\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "device = get_device()\n",
    "\n",
    "\n",
    "# 'microsoft/deberta-v2-xlarge'\n",
    "# \"bert-base-cased\"\n",
    "# \"bert-base-uncased\"\n",
    "for MODEL_NAME in ['microsoft/deberta-base']:\n",
    "    for pipeline in [['hyperlinks', 'mentions', 'hashtags', 'retweet', 'repetitions', 'emojis', 'smileys', 'spaces']\n",
    "                    ]:\n",
    "        \n",
    "        if TRAIN_DATA_PATH == \"../data/train_aug.csv\":\n",
    "            df = _load_dataset(TRAIN_DATA_PATH)\n",
    "        else:\n",
    "            df = _load_dataset(TRAIN_DATA_PATH)\n",
    "            df = _prepare_data(df, pipeline)\n",
    "\n",
    "        input_ids, attention_masks, labels = _create_tensors(df, MODEL_NAME)\n",
    "        train_dataloader, validation_dataloader = _create_dataloaders(input_ids, attention_masks, labels, BATCH_SIZE, \n",
    "                                                                    create_validation_set= True)\n",
    "\n",
    "        df = _load_dataset(TEST_DATA_PATH)\n",
    "        df = _prepare_data(df, pipeline)\n",
    "        input_ids, attention_masks, labels = _create_tensors(df, MODEL_NAME)\n",
    "        test_dataloader = _create_dataloaders(input_ids, attention_masks, labels, BATCH_SIZE, create_validation_set= False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "        model_class = Model(MODEL_NAME, NUM_EPOCHS, len(train_dataloader))\n",
    "        model, optimizer, lr_scheduler = model_class.get_model_optimizer_scheduler()\n",
    "        model = model.to(device)\n",
    "\n",
    "        training_loss = 0\n",
    "        val_loss = 0\n",
    "        val_f1 = 0\n",
    "\n",
    "        training_stats = []\n",
    "        try:\n",
    "            for epoch in range(NUM_EPOCHS):\n",
    "                print(f\"EPOCH {epoch+1}/{NUM_EPOCHS}\\n\")\n",
    "                model.train()\n",
    "                total_train_loss = 0\n",
    "\n",
    "                for step, batch in enumerate(train_dataloader):\n",
    "                    model.zero_grad()\n",
    "                    parameters = {\n",
    "                        \"input_ids\" : batch[0].to(device),\n",
    "                        \"attention_mask\" :  batch[1].to(device), \n",
    "                        \"labels\" : batch[2].to(device)\n",
    "                    }\n",
    "                    outputs = model(**parameters)\n",
    "\n",
    "                    loss = outputs.loss\n",
    "                    total_train_loss += loss.item()\n",
    "                    loss.backward()\n",
    "\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "                    optimizer.step()\n",
    "                    lr_scheduler.step()\n",
    "                    optimizer.zero_grad()\n",
    "                    # progress_bar.update(1)\n",
    "\n",
    "                    if step % 100 == 0 and step != 0:\n",
    "                        print(f\"BATCH {step}/{len(train_dataloader)}:\\tTraining loss({loss.item()})\")\n",
    "\n",
    "                training_stats.append({\n",
    "                    \"epoch\":epoch+1,\n",
    "                    \"training_loss\":total_train_loss/len(train_dataloader)\n",
    "                    })\n",
    "\n",
    "                total_val_loss = 0\n",
    "                metric = load_metric(\"f1\")\n",
    "\n",
    "                model.eval()\n",
    "                for batch in validation_dataloader:\n",
    "\n",
    "                    parameters = {\n",
    "                        \"input_ids\" : batch[0].to(device),\n",
    "                        \"attention_mask\" :  batch[1].to(device), \n",
    "                        \"labels\" : batch[2].to(device)\n",
    "                    }\n",
    "                    with torch.no_grad():\n",
    "                        outputs = model(**parameters)\n",
    "\n",
    "                    logits = outputs.logits\n",
    "                    loss = outputs.loss\n",
    "                    total_val_loss += loss.item()\n",
    "\n",
    "                    predictions = torch.argmax(logits, dim=-1)\n",
    "                    metric.add_batch(predictions=predictions, references=parameters[\"labels\"])\n",
    "\n",
    "                training_stats[epoch][\"validation_loss\"] = total_val_loss/len(validation_dataloader)\n",
    "                training_stats[epoch][\"validation_f1_score\"] = metric.compute()\n",
    "\n",
    "                print(f\"\\nAvg training loss:    {training_stats[epoch]['training_loss']}\")\n",
    "                print(f\"Avg validation loss:  {training_stats[epoch]['validation_loss']}\")\n",
    "                print(f\"F1 validation score:  {training_stats[epoch]['validation_f1_score']}\\n\")\n",
    "\n",
    "                training_loss = training_stats[epoch]['training_loss']\n",
    "                val_loss = training_stats[epoch]['validation_loss']\n",
    "                val_f1 = training_stats[epoch]['validation_f1_score']\n",
    "\n",
    "        except RuntimeError as e:\n",
    "            print(e)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        model = model.to(device)\n",
    "        testing_stats = []\n",
    "\n",
    "        try:\n",
    "            total_test_loss = 0\n",
    "            metric = load_metric(\"f1\")\n",
    "\n",
    "            model.eval()\n",
    "\n",
    "            for n, batch in enumerate(test_dataloader):\n",
    "\n",
    "                parameters = {\n",
    "                    \"input_ids\" : batch[0].to(device),\n",
    "                    \"attention_mask\" :  batch[1].to(device), \n",
    "                    \"labels\" : batch[2].to(device)\n",
    "                }\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(**parameters)\n",
    "                \n",
    "                logits = outputs.logits\n",
    "                loss = outputs.loss\n",
    "                total_test_loss += loss.item()\n",
    "\n",
    "                predictions = torch.argmax(logits, dim=-1)\n",
    "                metric.add_batch(predictions=predictions, references=parameters[\"labels\"])\n",
    "\n",
    "            testing_stats.append({\n",
    "                \"test_loss\": total_test_loss/len(test_dataloader),\n",
    "                \"test_f1_score\": metric.compute()\n",
    "            })\n",
    "\n",
    "            print(f\"\\nAvg test loss:  {testing_stats[0]['test_loss']}\")\n",
    "            print(f\"F1 test score:  {testing_stats[0]['test_f1_score']}\\n\")\n",
    "\n",
    "\n",
    "            results_dictionary[\"model_name\"].append(MODEL_NAME)\n",
    "            results_dictionary[\"pipeline\"].append(str(pipeline))\n",
    "            results_dictionary[\"training_loss\"].append(training_loss)\n",
    "            results_dictionary[\"validation_loss\"].append(val_loss)\n",
    "            results_dictionary[\"validation_f1\"].append(val_f1)\n",
    "            results_dictionary[\"test_loss\"].append(testing_stats[0]['test_loss'])\n",
    "            results_dictionary[\"test_f1\"].append(testing_stats[0]['test_f1_score'])\n",
    "            if TRAIN_DATA_PATH == \"../data/train_aug.csv\":\n",
    "                results_dictionary[\"augmented\"].append(\"yes\")\n",
    "            else:\n",
    "                results_dictionary[\"augmented\"].append(\"no\")\n",
    "\n",
    "\n",
    "        except RuntimeError as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "results_df = pd.DataFrame(results_dictionary)\n",
    "results_df.to_csv(\"../results/2nd_models_comparison.csv\", index=False)\n",
    "results_df.head(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check error cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import train_model_on_train_data\n",
    "\n",
    "\n",
    "# Parameters\n",
    "TRAIN_DATA_PATH = \"../data/train.csv\"\n",
    "MODEL_NAME = 'microsoft/deberta-base'\n",
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 1\n",
    "\n",
    "model, training_stats = train_model_on_train_data(TRAIN_DATA_PATH, MODEL_NAME, BATCH_SIZE, NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from model_preparation import set_seed\n",
    "from data_preparation import _load_dataset, _prepare_data, _create_dataloaders, _create_tensors\n",
    "\n",
    "\n",
    "data_path = \"../data/eval.csv\"\n",
    "model_name = 'microsoft/deberta-base'\n",
    "batch_size = 16\n",
    "create_validation_set = False\n",
    "SEED = 42\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "\n",
    "df = _load_dataset(data_path)\n",
    "\n",
    "final_df = df[\"tweet\"].copy()\n",
    "\n",
    "pipeline = ['hyperlinks', 'mentions', 'hashtags', 'retweet', 'repetitions', 'emojis', 'smileys', 'spaces']\n",
    "df = _prepare_data(df, pipeline)\n",
    "\n",
    "final_df = pd.concat([final_df, df[[\"text\",\"label\"]]], axis = 1)\n",
    "\n",
    "input_ids, attention_masks, labels = _create_tensors(df, model_name)\n",
    "dataloaders = _create_dataloaders(input_ids, attention_masks, labels, batch_size, create_validation_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions import get_device\n",
    "from datasets import load_metric\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "device = get_device()\n",
    "model = model.to(device)\n",
    "\n",
    "pred_list = np.array([])\n",
    "\n",
    "test_dataloader = dataloaders\n",
    "\n",
    "testing_stats = []\n",
    "\n",
    "try:\n",
    "    total_test_loss = 0\n",
    "    metric = load_metric(\"f1\")\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    for n, batch in enumerate(test_dataloader):\n",
    "\n",
    "        parameters = {\n",
    "            \"input_ids\" : batch[0].to(device),\n",
    "            \"attention_mask\" :  batch[1].to(device), \n",
    "            \"labels\" : batch[2].to(device)\n",
    "        }\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**parameters)\n",
    "        \n",
    "        logits = outputs.logits\n",
    "        loss = outputs.loss\n",
    "        total_test_loss += loss.item()\n",
    "\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        metric.add_batch(predictions=predictions, references=parameters[\"labels\"])\n",
    "\n",
    "        if input_ids[n*32:(n+1)*32].equal(parameters[\"input_ids\"].cpu()):\n",
    "            pred_list = np.append(pred_list,predictions.cpu().numpy())\n",
    "\n",
    "    testing_stats.append({\n",
    "        \"test_loss\": total_test_loss/len(test_dataloader),\n",
    "        \"test_f1_score\": metric.compute()\n",
    "    })\n",
    "\n",
    "    print(f\"\\nAvg test loss:  {testing_stats[0]['test_loss']}\")\n",
    "    print(f\"F1 test score:  {testing_stats[0]['test_f1_score']}\\n\")\n",
    "\n",
    "except RuntimeError as e:\n",
    "    print(e)\n",
    "\n",
    "final_df[\"prediction\"] = pred_list\n",
    "final_df[\"match\"] = final_df.apply(lambda row: \"\" if row[\"label\"] == row[\"prediction\"] else \"NO\", axis=1)\n",
    "\n",
    "# final_df.to_csv(\"predictions.csv\",index=False)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "final_df[final_df[\"match\"]==\"NO\"].head(40)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a97517a1f2e44bae43a0d1adced36113c0aa0bace4a2569b7d21fd338b4eff8a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
